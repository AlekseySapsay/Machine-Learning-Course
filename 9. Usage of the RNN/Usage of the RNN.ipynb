{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Usage of the RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the last lecture, we have introduced general idea of a recurrent neural network - preserving previous states of the network (hidden and cell states). We have dissected the most basic RNN and more complex LSTM, which utilizes *gates* and *a cell state*.\n",
    "\n",
    "Then we have learned how to implement simple RNN and LSTM in Python using machine learning library Keras. Let's recollect it again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#example code \n",
    "from keras.models import Sequential,load_model\n",
    "from keras.layers import Dense,LSTM,Dropout\n",
    "from keras import optimizers\n",
    "\n",
    "batch_size=64 #during training a lot of training examples (batch) are taken and gradient (weight changes) is averaged\n",
    "timesteps=50 #length of training sequence\n",
    "input_length=26 #length of one element of sequence\n",
    "hid_neurons=128 #number of hidden neurons in LSTM\n",
    "output_size=3 #output size of neural network\n",
    "\n",
    "model=Sequential()\n",
    "model.add(LSTM(hid_neurons,batch_input_shape=(batch_size,timesteps,input_length)))\n",
    "model.add(Dense(output_size))\n",
    "model.compile(loss='categorical_crossentropy',optimizer=optimizers.RMSprop(lr=0.001),metrics=['accuracy'])\n",
    "#categorical_crossentropy is used if y-data consists from one-hot-vectors (1 example - 1 class)\n",
    "#binary_crossentropy is used for multilabeled data (1 example - a lot of classes)\n",
    "#RMSprop is popular optimizer, which is more advanced than basic vanilla gradient descent\n",
    "#Alternatively, you can use optimizer Adam by specifying optimizer='adam' or optimizer=optimizers.Adam(lr=learning_rate)\n",
    "\n",
    "model.fit(X_data,Y_data,epochs=50,batch_size=64,shuffle=False) #training \n",
    "#WARNING! batch_size must be equal to batch_size used while constructing net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us have N different classes and each training example is corresponding to one class. In that case y-data can be represented as a collection of one-hot vectors. One-hot vector representing class with number i is a vector $\\mathbf{x}$ of size N, which elements are defined as:\n",
    "$$x_k=\n",
    "\\begin{cases}\n",
    "1, k=i \\\\\n",
    "0, k\\neq i\n",
    "\\end{cases}$$\n",
    "**Example:** There are 4 different classes and we encode the second one as one-hot vector (0,1,0,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also have started exploring what RNN is good at. The first task was classification (prediction of parabola's coefficients by a sequence of y-coordinates). \n",
    "\n",
    "By analogy, you can create data (see table below) of sequences and corresponding one-hot vectors and train RNN to classify sequences in different categories.\n",
    "\n",
    "|word (sequence of letters)|positive/negative/neutral|one-hot vector|\n",
    "|--------------------------|-----------------|--------------|\n",
    "|excellent|positive|1 0 0|\n",
    "|the worst|negative|0 1 0|\n",
    "|green|neutral|0 0 1|\n",
    "|astonishing|positive|1 0 0|\n",
    "|cloudy|neutral|0 0 1|\n",
    "|disgusting|negative|0 1 0|\n",
    "\n",
    "**Homework**: Create LSTM neural network which classifies something and train it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can classify sequences into any classes we want as long as they can be deduced from present data. Imagine having a very long sequence {$x_n$} of elements (letters, words, notes) and taking mini-sequence with t elements: $x_{i+1},x_{i+2},...,x_{i+t}$. \n",
    "\n",
    "We can \"classify\" each mini-sequence by next character $x_{i+t+1}$ exactly after this mini-sequence. In that case, x-data is going to look like a collection of sequences of elements (each element can be a single number, one-hot vector or just a vector), and y-data is going to consist from one-hot vectors, which represent next character in the sequence after the corresponding mini-sequence. \n",
    "\n",
    "Now imagine the following course of actions:\n",
    "1. Let us have 5 different classes: \"A\", \"B\", \"C\", \"D\", and \"E\"\n",
    "2. Let us have a mini-sequence \"A B C\"\n",
    "3. A trained neural network, essentially, outputs probability distribution of the next character, which appears as a vector of size 5: (0.01, 0.02, 0.01, 0.90, 0.06). In a general case, let us assign output as $(y_1,y_2,...,y_n)\\equiv\\mathbf{y}$. Each number $y_i$ can be described as a probability of next character being the corresponding class. For example, class \"D\" has 90% probability of showing after \"A B C\" sequence.\n",
    "4. Now we have two possible ways of actually determining the next character. \n",
    "    * The first one is picking the class with the highest probability (in this example \"D\"). \n",
    "    * The second one is taking a class $i$ with probability proportional to $y_i^{1/\\tau}$, where $\\tau$ is called *temperature*. The higher temperature, the more *diverse* and at the same time more *random* generated sequence is going to be. By the way, as $\\tau$ approaches zero, the second way of generation gets closer to the first one.\n",
    "5. After we have picked an element, for example, \"D\", initial mini-sequence transforms into \"A B C D\". Since the neural network takes 3 last characters as an input in this example, last 3 characters - \"B C D\" can be taken and fed to the neural network, which outputs probability distribution for the next character. Now, we can repeat steps 3-4 to get a sequence of the desired length. Thus, the neural network becomes a **sequence generating machine**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text can be represented as a sequence of words or letters. Let's start from generation of text letter-by-letter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Letter-by-letter generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, we need to specify a data source. Since we are generating a text as a sequence of letters, we have much more freedom in choosing text. On the other hand, we can't train LSTM on very long sequences because it's computationally infeasible task. As a result, a quite short (in terms of whole text) sequence from 100 letters represents \"memory cap\" of LSTM, which means that LSTM probably forgets all context after 100 symbols. That is why a generated text is quite repetitive and doesn't have a lot of deep meaning. Nonetheless, I have chosen Latex code of one chapter (topology.tex) of the web-based project on algebraic stacks and algebraic geometry (https://github.com/stacks/stacks-project/) to showcase the strength of LSTM in remembering the high-level syntax of Latex code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Secondly, we need to preprocess raw .tex code. This process consists of several steps:\n",
    "* Loading text\n",
    "\n",
    "```python\n",
    "raw_text=open('your_directory/your_file.txt').read()\n",
    "```\n",
    "\n",
    "* Creating dictionary character-integer, which sets one integer corresponding to one and only one character. Length of such dictionary is equal to the number of distinct characters and, later one, it will be called n_vocab.\n",
    "\n",
    "```python\n",
    "chars = sorted(list(set(raw_text)))\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars)) #the dictionary which converts a characters to an integer\n",
    "int_to_char = dict((i, c) for i, c in enumerate(chars)) #the dictionary which converts an integer to a characters\n",
    "n_vocab = len(chars) #number of distinct characters\n",
    "```\n",
    "\n",
    "* Creating training examples, which consist of x and y data, in following steps:\n",
    "    1. Taking mini-sequences of letters from the text. Each mini-sequence is shifted from the previous one by one letter. If a sequence is, for example, \"ABCDEF\", we can get 3 training examples having the form (sequence, next letter): (\"ABC\",\"D\"); (\"BCD\",\"E\"); (\"CDE\",\"F\").\n",
    "    2. Converting each sequence into a collection of one-hot vectors, corresponding to according letters and the one-hot vector responsible for next letter.\n",
    "    \n",
    "```python\n",
    "corpus_size=10000\n",
    "seq_length = 100\n",
    "\n",
    "global dataX,dataY\n",
    "\n",
    "for i in range((n_chars-seq_length)//corpus_size):\n",
    "    dataX=np.zeros((corpus_size,seq_length,n_vocab)) \n",
    "    dataY=np.zeros((corpus_size,n_vocab))\n",
    "    for k in range(corpus_size):\n",
    "        seq=raw_text[i*corpus_size+k:i*corpus_size+k+seq_length]\n",
    "        final_char=raw_text[i*corpus_size+k+seq_length]\n",
    "        for j in range(seq_length):\n",
    "            dataX[k,j,char_to_int[seq[j]]]=1            \n",
    "        dataY[k,char_to_int[final_char]]=1\n",
    "        \n",
    "    model.fit(dataX,dataY,batch_size=50,epochs=1,verbose=True,shuffle=False)\n",
    "```\n",
    "\n",
    "**Warning:** Be careful with a size of dataX array because it can exceed memory limit and kernel can die (even Google Colab memory isn't enough)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there isn't enough memory to convert whole text into x-data, we can break data into pieces, which were called corpus. Each corpus consists of 10000 sequences, which neural network trains on for one epoch. After training new corpus is created and the process is repeated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generation of a text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While we were training a neural network, it had a sufficient batch size for stable training. However, during generating we don't need to provide a batch of sequences. Instead, we need to generate next character from one mini-sequence of last characters. To deal with this problem, we need to create *generating* neural network with batch size equal to 1.\n",
    "Finally, we can easily generate texts about algebraic geometry using following code:\n",
    "```python\n",
    "temp=1.5 #INVERSE temperature\n",
    "offset=0\n",
    "s=raw_text[offset:offset+seq_length] #taking starting sequence from original text\n",
    "for j in range(5000):\n",
    "    if j%100==0:\n",
    "        print(j)\n",
    "    x=np.zeros((1,seq_length,n_vocab)) #creating mini-sequence from 100 last characters\n",
    "    for i in range(seq_length):\n",
    "        x[0,i,char_to_int[s[-seq_length+i]]]=1\n",
    "    \n",
    "    y=model2.predict(x)[0]\n",
    "    s+=int_to_char[np.random.choice(n_vocab,p=y**temp/np.sum(y**temp))] #choose character and append it to generated sequence\n",
    "    \n",
    "print(s)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trained model can be found at [Google Drive](https://drive.google.com/open?id=1q7-IfXGlwKj9qfPSbyieUUT6bQ9FjBEO). \n",
    "\n",
    "I have generated several texts with different (inverse) temperatures, which are stored at [github.com](https://github.com/romasoletskyi/Machine-Learning-Course/tree/master/9.%20Usage%20of%20the%20RNN/Latex%20generation)\n",
    "\n",
    "As expected, low temperature $\\tau=1/2$ creates very repetitive text, enriched with \"topological rings\". At $\\tau=0$, which corresponds to picking the most likely character every time, text degrades into infinite loop \"of topological ring of topological ring of topological ring...\". On the other hand, high temperature $\\tau=1$ creates text with a lot of grammatic errors, which sometimes becomes gibberish.\n",
    " \n",
    "Temperature equal to $\\tau=2/3$ becomes \"a golden middle\", where a generated text is diverse and grammatically correct simultaneously. During generation of 10000 symbols, Latex compiler found only 6 errors. They were only \"long range\" errors that require remembering from several paragraphs to the whole text (e.g. not ending long lemmas and proofs with according commands)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Music generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Music generation is pretty similar to letter-by-letter text generation, except we get data differently. The only major difference is that music notes are tightly connected with the conception of frequency - continuous measurable physics value. Based on that, we can propose to encode notes, not as one-hot vectors, but simply like index number of a class divided by the total number of classes. This makes neural network smaller and allows to store all data in RAM and train net on a large pull of shuffled sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data preprocessing code was mainly inspired by [this article](https://towardsdatascience.com/how-to-generate-music-using-a-lstm-neural-network-in-keras-68786834d4c5). We can use library music21, which must be installed on your local machine or on Google Colab. It helps us to parse MIDI files into a sequence of notes and chords, which are then enumerated and normalized by dividing by the total number of different notes/chords.\n",
    "I have used MIDI files of Beethoven piano compositions (the main type is sonata) and, as a result, I got the collection of notes/chords. Data  was finally processed using this code:\n",
    "\n",
    "``` python\n",
    "x_data=[]\n",
    "y_data=[]\n",
    "\n",
    "for i in range(len(data_notes)-seq_length):            \n",
    "      x_data.append([note_to_int[data_notes[i+k]]/n_vocab for k in range(seq_length)])\n",
    "      one_hot=np.zeros(n_vocab)\n",
    "      one_hot[note_to_int[data_notes[i+seq_length]]]=1\n",
    "      y_data.append(one_hot)\n",
    "\n",
    "x_data=np.array(x_data).reshape((len(x_data),seq_length,classes_number))\n",
    "y_data=np.array(y_data).reshape((len(y_data),n_vocab)) \n",
    "\n",
    "x_data=np.copy(x_data[:(len(x_data)//batch_size)*batch_size])\n",
    "y_data=np.copy(y_data[:(len(y_data)//batch_size)*batch_size])\n",
    "```\n",
    "\n",
    "A neural network was trained on the whole data and after each epoch model was saved if it was better then a previously saved version:\n",
    "\n",
    "```python\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "filepath = \"drive/Colab/Music generator/model-{epoch:02d}-{loss:.4f}-bigger.h5\"\n",
    "checkpoint = ModelCheckpoint(\n",
    "        filepath,\n",
    "        monitor='loss',\n",
    "        verbose=0,\n",
    "        save_best_only=True,\n",
    "        mode='min'\n",
    ")\n",
    "callbacks_list = [checkpoint]\n",
    "    \n",
    "history=model.fit(x_data,y_data,epochs=5,batch_size=128, callbacks=callbacks_list)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generation of music"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training, we need to create generating a neural network with batch size equal to one. Generating of music is a bit more complicated than generating a text, however, library music21 greatly helps here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp=2\n",
    "prediction_output=[]\n",
    "ind=int((len(x_data)*np.random.random()))\n",
    "for i in range(seq_length):\n",
    "    prediction_output.append(x_data[ind][i])\n",
    "for i in range(120): #120 - number of generated notes/chords\n",
    "    if i%50==0:\n",
    "        print(i)\n",
    "    y=model2.predict(np.array(prediction_output[-seq_length:]).reshape(1,seq_length,classes_number),batch_size=1)[0]\n",
    "    prediction_output.append(np.random.choice(n_vocab,p=y**(1/temp)/np.sum(y**(1/temp))))\n",
    "    \n",
    "prediction_output=[int_to_note[x] for x in prediction_output[seq_length:]]\n",
    "    \n",
    "offset = 0\n",
    "output_notes = []\n",
    "# create note and chord objects based on the values generated by the model\n",
    "for pattern in prediction_output:\n",
    "    # pattern is a chord\n",
    "    if ('.' in pattern) or pattern.isdigit():\n",
    "        notes_in_chord = pattern.split('.')\n",
    "        notes = []\n",
    "        for current_note in notes_in_chord:\n",
    "            new_note = note.Note(int(current_note))\n",
    "            new_note.storedInstrument = instrument.Piano()\n",
    "            notes.append(new_note)\n",
    "        new_chord = chord.Chord(notes)\n",
    "        new_chord.offset = offset\n",
    "        output_notes.append(new_chord)\n",
    "    # pattern is a note\n",
    "    else:\n",
    "        new_note = note.Note(pattern)\n",
    "        new_note.offset = offset\n",
    "        new_note.storedInstrument = instrument.Piano()\n",
    "        output_notes.append(new_note)\n",
    "    # increase offset each iteration so that notes do not stack\n",
    "    offset += 0.5\n",
    "    \n",
    "midi_stream = stream.Stream(output_notes)\n",
    "midi_stream.write('midi', fp='drive/Colab/Music generator/generated_beeth2.mid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, a MIDI file can be converted into a mp3 file using any online converter. You can listen to some generated samples on [github.com](https://github.com/romasoletskyi/Machine-Learning-Course/tree/master/9.%20Usage%20of%20the%20RNN/Music%20generation). The trained model can be found on [Google Drive](https://drive.google.com/open?id=1cbn5yHZfii2PcMMKiz1VSd1ajV0r_5rd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Letter-by-letter generation Colab notebook](https://drive.google.com/file/d/1hZlg4gy9Hv9Zy57ke7oGmJKx91KGoewL/view?usp=sharing)\n",
    "2. [Latex texts generation Colab notebook](https://drive.google.com/file/d/1mzoSJ8aEZLlz0ZYuNQNJnihFjFmqygYc/view?usp=sharing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
